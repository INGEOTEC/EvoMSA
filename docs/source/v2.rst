.. _v2:

EvoMSA 2.0
====================================
.. image:: https://github.com/INGEOTEC/EvoMSA/actions/workflows/test.yaml/badge.svg
		:target: https://github.com/INGEOTEC/EvoMSA/actions/workflows/test.yaml

.. image:: https://coveralls.io/repos/github/INGEOTEC/EvoMSA/badge.svg?branch=develop
		:target: https://coveralls.io/github/INGEOTEC/EvoMSA?branch=develop

.. image:: https://badge.fury.io/py/EvoMSA.svg
		:target: https://badge.fury.io/py/EvoMSA

.. image:: https://dev.azure.com/conda-forge/feedstock-builds/_apis/build/status/evomsa-feedstock?branchName=main
	    :target: https://dev.azure.com/conda-forge/feedstock-builds/_build/latest?definitionId=16466&branchName=main

.. image:: https://img.shields.io/conda/vn/conda-forge/evomsa.svg
		:target: https://anaconda.org/conda-forge/evomsa

.. image:: https://img.shields.io/conda/pn/conda-forge/evomsa.svg
		:target: https://anaconda.org/conda-forge/evomsa

.. image:: https://readthedocs.org/projects/evomsa/badge/?version=docs
		:target: https://evomsa.readthedocs.io/en/docs/?badge=docs

.. image:: https://colab.research.google.com/assets/colab-badge.svg
		:target: https://colab.research.google.com/github/INGEOTEC/EvoMSA/blob/master/docs/Quickstart.ipynb	   



EvoMSA is a stack generalization algorithm specialized in text classification
problems. A text classifier :math:`c`, can be seen as a composition of two functions,
i.e., :math:`c \equiv g \circ m`; where :math:`m` transforms the text into a vector space,
i.e., :math:`m: \text{text} \rightarrow \mathbb R^d` and :math:`g` is the classifier
(:math:`g: \mathbb R^d \rightarrow \mathbb N`) or regressor (:math:`g: \mathbb R^d \rightarrow \mathbb R`).
Stack generalization is a technique to combine classifiers (regressors) to produce 
another classifier (regressor) responsible for making the prediction. 

EvoMSA 2.0 removes two text representations, i.e., functions :math:`m`, particularly 
the sentiment lexicon-based model, and the aggressiveness model. It was decided 
to remove them because these models are the ones that require more work to be 
implemented in another language and, on the other hand, are the ones that 
contribute less to the performance of the algorithm. However, EvoMSA 2.0 increments
the number of human-annotated models, the emoji models, and introduces a new
model, namely keyword models.

EvoMSA 2.0 increments the number of supported languages, currently it
supports Arabic (ar), Catalan (ca), German (de), English (en), 
Spanish (es), French (fr), Hindi (hi), Indonesian (in), 
Italian (it), Japanese (ja), Korean (ko), Dutch (nl),
Polish (pl), Portuguese (pt), Russian (ru), Tagalog (tl), 
Turkish (tr), and Chinese (zh). It also provides pre-trained models
that include the bag-of-words text representations, emoji, 
and keyword models. These models were trained on Twitter data. 

The other enhancement is on the implementation; it is cleaner in 
EvoMSA 2.0 than in the previous version. There are three main 
classes :py:class:`BoW`, :py:class:`TextRepresentations`, 
and :py:class:`StackGeneralization`. 
:py:class:`BoW` and :py:class:`TextRepresentations` are text
classifiers; :py:class:`BoW` is the parent of :py:class:`TextRepresentations`.
The stack generalization technique is implemented in 
:py:class:`StackGeneralization`.

:py:class:`BoW`
================================

:py:class:`BoW` is a text classifier :math:`c` with signature 
:math:`c \equiv g \circ m`, where :math:`m` stands for the 
bag-or-words representation and :math:`g` is the classifier
(the default is a linear Support Vector Machine).

The classifier :math:`g` is trained on dataset :math:`\mathcal D`
of pairs (:math:`x`, :math:`y`), where :math:`x` is a text and
:math:`y` is the label associated with the text. The bag-of-words
representation :math:`m` is a pre-trained model used as the weighting 
scheme term frequency inverse document frequency (TFIDF);
:math:`m` was trained on 524,288 (:math:`2^{19}`) tweets
randomly selected.

The bag-of-words representation used is described in
"`A Simple Approach to Multilingual Polarity Classification in Twitter <https://www.sciencedirect.com/science/article/abs/pii/S0167865517301721>`_. 
Eric S. Tellez, Sabino Miranda-Jiménez, Mario Graff, 
Daniela Moctezuma, Ranyart R. Suárez, Oscar S. Siordia. 
Pattern Recognition Letters" and
"`An Automated Text Categorization Framework based 
on Hyperparameter Optimization <https://www.sciencedirect.com/science/article/abs/pii/S0950705118301217>`_. Eric S. Tellez, Daniela Moctezuma, 
Sabino Miranda-Jímenez, Mario Graff. 
Knowledge-Based Systems Volume 149, 1 June 2018."

Bag-of-Words Representation
--------------------------------

The core idea of a bag of words is that after the text is normalized and tokenized, each 
token :math:`t` is associated with a vector :math:`\mathbf{v_t} \in \mathbb R^d` 
where the :math:`i`-th component, i.e., :math:`\mathbf{v_t}_i`, contains the IDF value of 
the token :math:`t` and :math:`\forall_{j \neq i} \mathbf{v_t}_j=0`. 
The set of vectors :math:`\mathbf v` corresponds to the vocabulary,
there are :math:`d` different tokens in the vocabulary, and by definition
:math:`\forall_{i \neq j} \mathbf{v_i} \cdot \mathbf{v_j} = 0`, where
:math:`\mathbf{v_i} \in \mathbb R^d`, :math:`\mathbf{v_j} \in \mathbb R^d`,
and :math:`(\cdot)` is the dot product. It is worth mentioning that any 
token outside the vocabulary is discarded. The vocabulary size of the 
pre-trained bag-of-words representations is :math:`d=2^{14}`,
i.e., there are 16,384 different tokens for each language.

Using this notation, a text :math:`x` is represented by the sequence
of its tokens, i.e., :math:`(t_1, t_2, \ldots)`; the sequence 
can have repeated tokens, e.g., :math:`t_j = t_k`. 
Then each token is associated with its 
respective vector :math:`\mathbf v` (keeping the repetitions),
i.e., :math:`(\mathbf{v_{t_1}}, \mathbf{v_{t_2}}, \ldots)`.
Finally, the text :math:`x` is represented as: 

.. math:: 
	\mathbf x = \frac{\sum_t \mathbf{v_t}}{\mid\mid \sum_t \mathbf{v_t} \mid\mid},

where the sum goes for the sequence, :math:`\mathbf x \in \mathbb R^d`,
and :math:`\mid\mid \mathbf w \mid\mid` is the Euclidean norm of vector 
:math:`\mathbf w`. The term frequency is implicitly computed in the sum 
because the process allows token repetitions.

This process is illustrated by representing the text *good morning*
in the pre-trained bag-of-words model. The first step is to import 
and initialize the class :py:class:`BoW` as done in the following 
instructions.

>>> from EvoMSA import BoW
>>> bow = BoW(lang='en')

The method :py:attr:`BoW.transform` receives a list of text to be represented
in the vector space of the bag of words. It returns a sparse matrix where 
the number of rows corresponds to the number of texts transformed and the columns
are the vocabulary. The following instruction processes the text *good morning*
and stored it in a variable :py:attr:`X`.

>>> X = bow.transform(['good morning'])
>>> X
<1x16384 sparse matrix of type '<class 'numpy.float64'>'
	with 35 stored elements in Compressed Sparse Row format>

The non-zero components are found in :py:attr:`X.indices`

>>> X.indices
array([   5,   10,   20,   25,   30,   37,   41,   43,   53,   64,  138,
        148,  166,  175,  217,  220,  272,  394,  438,  461,  484,  524,
        538,  565,  592,  654,  655,  662,  748, 1198, 1227, 1365, 1594,
       1636, 1898], dtype=int32)

However, one might wonder which token corresponds to each component; 
this information is in :py:attr:`BoW.names`. For example, 
the tokens associated with components 748 and 1898 are: *good*
and *morning*, as can be seen below. 

>>> bow.names[748], bow.names[1898]
('good', 'morning')

The IDF values associate to each token are in the dictionary 
:py:attr:`BoW.bow.token_weight`, e.g., the IDF value of text 
*morning* is 

>>> bow.bow.token_weight[1898]
6.354566724835741

Nonetheless, the value that the component 1898 has in the variable :py:attr:`X` is
0.2714 because the vector that represents *good morning* has been
normalized to have a unit length. 

Classifier
--------------------------------

Once the texts are in a vector space, then any classifier that works 
with vector can be used; the one used by default is a linear Support Vector Machine.

To illustrate the process of creating a text classifier with :py:class:`BoW`, the 
following dataset will be used. 

>>> from EvoMSA import base
>>> from microtc.utils import tweet_iterator
>>> from os.path import join, dirname
>>> tweets = join(dirname(base.__file__), 'tests', 'tweets.json')
>>> D = list(tweet_iterator(tweets))

The dataset stored in :py:attr:`D` is a toy sentiment analysis dataset,
in Spanish, with four labels, positive, negative, neutral, and none. 
It is a list of dictionaries where the dictionary has two keys *text* 
and *klass*; the former has the text and the latter the label. 

The text classifier is trained with the following instruction. 

>>> bow = BoW(lang='es').fit(D)

where the language (:py:attr:`lang`) is set to Spanish (es), and 
:py:attr:`fit` receives the labeled dataset. 

The method :py:attr:`BoW.predict` is used to predict the label 
of a list of texts. For example, the label of the text *buenos días* (*good morning*)
is computed as:

>>> bow.predict(['buenos días'])
array(['P'], dtype='<U4')

where label 'P' corresponds to positive. 

There are scenarios where it is more important to estimate the value(s) 
used to classify a particular instance; in the case of SVM, 
this is known as the decision function, and in the case of a 
Naive Bayes classifier, this is the probability of each class. 
This information can be found in :py:attr:`BoW.decision_function`
as can be seen in the following code.

>>> bow.decision_function(['buenos días'])
array([[-1.5749252 , -1.0800899 , -0.21577256,  0.56413664]])


:py:class:`TextRepresentations`
================================


:py:class:`StackGeneralization`
================================


Documentation
================================

.. autoclass:: EvoMSA.evodag.BoW
   :members:

.. autoclass:: EvoMSA.evodag.TextRepresentations
   :members:

.. autoclass:: EvoMSA.evodag.StackGeneralization
   :members:
